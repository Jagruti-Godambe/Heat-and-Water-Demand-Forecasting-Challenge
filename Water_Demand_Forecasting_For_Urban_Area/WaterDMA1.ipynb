{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from WaterDMA1 import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Water data set paths\n",
    "WaterDMA_1_Number_of_Meters_path = '/Users/jagrutigodambe/Desktop/Data/water/WaterDMA_1_Number_of_Meters.csv'\n",
    "Training_WaterDMA_1_path = '/Users/jagrutigodambe/Desktop/Data/water/Training_WaterDMA_1.csv'\n",
    "Testing_WaterDMA_1_path = '/Users/jagrutigodambe/Desktop/Data/water/Testing_WaterDMA_1.csv'\n",
    "Weather_data_path = '/Users/jagrutigodambe/Desktop/Data/weather/Weather_Bronderslev_20152022.csv'\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data preprocessing with file paths\n",
    "prepro = Water_data_preprocessing(Training_WaterDMA_1_path,\n",
    "                        WaterDMA_1_Number_of_Meters_path,\n",
    "                        Weather_data_path)\n",
    "\n",
    "# Fit the preprocessing on training data and transform it, then apply the same transformation to testing data\n",
    "train_data = prepro.fit()\n",
    "test_data = prepro.transform(Testing_WaterDMA_1_path)\n",
    "\n",
    "train_data_scaling_info = train_data[['comsumption','meters']]\n",
    "test_data_scaling_info = test_data[['comsumption','meters']]\n",
    "\n",
    "train_data = train_data.drop(['comsumption','meters'], axis=1)\n",
    "test_data = test_data.drop(['comsumption','meters'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": 
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a box plot for outlier detection\n",
    "plt.figure(figsize=(10, 6)) \n",
    "sns.boxplot(y=train_data['Per_meter_comsumption_with_inter']) \n",
    "plt.title('Box Plot of Per Meter Consumption with Inter')\n",
    "plt.ylabel('Per Meter Consumption')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: 0.00504349748810322, Q3: 0.014305854200097142, IQR: 0.009262356711993922\n",
      "0.028199389268088022\n"
     ]
    }
   ],
   "source": [
    "# Calculate quartiles, IQR, and bounds for outlier detection\n",
    "data = train_data['Per_meter_comsumption_with_inter']\n",
    "Q1 = np.percentile(data, 25)\n",
    "Q3 = np.percentile(data, 75)\n",
    "\n",
    "# Calculate IQR\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify outliers\n",
    "print(f\"Q1: {Q1}, Q3: {Q3}, IQR: {IQR}\")\n",
    "\n",
    "print(upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], Name: Per_meter_comsumption_with_inter, dtype: float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the threshold using clip method\n",
    "train_data = pd.DataFrame(train_data)\n",
    "# Define the threshold (upper bound)\n",
    "threshold = 0.0281\n",
    "\n",
    "train_data['Per_meter_comsumption_with_inter'] = train_data['Per_meter_comsumption_with_inter'].clip(upper=threshold)\n",
    "\n",
    "if 'Per_meter_comsumption_with_inter' in train_data.columns:\n",
    "    filtered_data = train_data[train_data['Per_meter_comsumption_with_inter'] > 0.0281]\n",
    "    print(\"Filtered DataFrame:\")\n",
    "    display(filtered_data['Per_meter_comsumption_with_inter'])\n",
    "else:\n",
    "    print(\"Column 'consumption' does not exist in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/31200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31200/31200 [00:02<00:00, 10697.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create sequences and labels\n",
    "\n",
    "sequence_length_x = 192\n",
    "sequence_length_y = 24 # y is label\n",
    "\n",
    "sequences, labels , train_scaling_info  = create_sequences(train_data, sequence_length_x, sequence_length_y, train_data_scaling_info)\n",
    "\n",
    "train_X, train_y, train_scale_info, test_X, test_y, test_scale_info = train_test_split(sequences, labels , train_scaling_info, train_size = 0.75)\n",
    "\n",
    "\n",
    "def predict_inverse(model ,data):#, scaling_info):\n",
    "    with torch.no_grad():\n",
    "       data = torch.Tensor(data)\n",
    "       prediction = model(data)\n",
    "    return pd.DataFrame(prediction.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters, create data loaders, and initialize the model and optimizer\n",
    "input_size = 29  # Feature dimension\n",
    "sequence_length = 192\n",
    "\n",
    "hidden_size = 256\n",
    "num_layers = 4\n",
    "\n",
    "output_size = 24\n",
    "\n",
    "\n",
    "num_epochs = 500\n",
    "batch_size = 32\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "device = get_device()\n",
    "print(device)\n",
    "\n",
    "\n",
    "class HeatData(Dataset):\n",
    "\n",
    "    def __init__(self, sequences, labels):\n",
    "      self.sequences = sequences\n",
    "      self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "      return self.sequences.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      return torch.Tensor(self.sequences[idx]), torch.Tensor(self.labels[idx])\n",
    "\n",
    "\n",
    "heatData_train = HeatData(train_X, train_y)\n",
    "heatData_val = HeatData(test_X, test_y)\n",
    "\n",
    "train_dataloader = DataLoader(heatData_train, batch_size=batch_size, shuffle = True)\n",
    "test_dataloader = DataLoader(heatData_val, batch_size=batch_size, shuffle = False)\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, output_size, num_layers)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "model = model.to('mps')\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
 
   "source": [
    "# Train the model \n",
    "list_train_loss = []\n",
    "list_val_loss = []\n",
    "\n",
    "prev_val_loss = 1000\n",
    "epochs = 50\n",
    "for e in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    for data, labels in train_dataloader:\n",
    "        # Transfer Data to GPU if available\n",
    "        \n",
    "\n",
    "        data, labels = data.to('mps'), labels.to('mps')\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward Pass\n",
    "        target = model(data)\n",
    "        # Find the Loss\n",
    "        loss = criterion(target,labels)\n",
    "        # Calculate gradients\n",
    "        loss.backward()\n",
    "        # Update Weights\n",
    "        optimizer.step()\n",
    "    list_train_loss.append(loss.item())\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    model.eval()     # Optional when not using Model Specific layer\n",
    "    for data, labels in test_dataloader:\n",
    "        # Transfer Data to GPU if available\n",
    "        \n",
    "        data, labels = data.to('mps'), labels.to('mps')\n",
    "        # Forward Pass\n",
    "        target = model(data)\n",
    "        # Find the Loss\n",
    "        val_loss = criterion(target, labels)\n",
    "        # Calculate Loss\n",
    "        valid_loss += val_loss.item()\n",
    "    valid_loss =  valid_loss/len(test_dataloader)\n",
    "    list_val_loss.append(valid_loss)\n",
    "\n",
    "    if prev_val_loss > valid_loss:\n",
    "        # updating loss\n",
    "        prev_val_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model_watertest{}.pth'.format(e))\n",
    "\n",
    "    print(f'Epoch {e+1} \\t\\t Training Loss: { loss.item()} \\t\\t Validation Loss: { valid_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained LSTM model weights\n",
    "model = LSTMModel(input_size, hidden_size, output_size, num_layers)\n",
    "model.load_state_dict(torch.load('model_watertest{}.pth'.format(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions and calculate MAE and MAPE.\n",
    "predictions = predict_inverse(model, test_X)\n",
    "prediction_for_all_meter = pd.DataFrame(test_scale_info[:,:,1]) * predictions\n",
    "\n",
    "mean_absolute_error(test_scale_info[:,:,0],prediction_for_all_meter.to_numpy())\n",
    "mean_absolute_percentage_error(test_scale_info[:,:,0],prediction_for_all_meter.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and fill missing values in the test data using the model.\n",
    "def test_pred(model, test_data):\n",
    "    torch.no_grad()\n",
    "    test_data = test_data.reset_index()\n",
    "    \n",
    "    idx = test_data[test_data.Per_meter_comsumption_with_inter.isna()].timestamp.dt.date.drop_duplicates(keep='first').index.tolist()\n",
    "    print(idx)\n",
    "    for i in idx:\n",
    "        pre = model(torch.Tensor((test_data.loc[i-192\n",
    "              :i-1]).drop('timestamp', axis=1).values).reshape(1,192,29))\n",
    "        test_data.loc[i :i+23,'Per_meter_comsumption_with_inter'] = pre.detach().numpy().reshape(-1,1)\n",
    "    \n",
    "    return test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[192, 384, 576, 768, 960, 1152, 1344, 1536, 1728, 1920, 2112, 2304, 2496, 2688, 2880, 3072, 3264, 3456, 3648, 3840, 4032, 4224, 4416, 4608]\n"
     ]
    }
   ],
   "source": [
    "preTest = test_pred(model, test_data)\n",
    "preTest= preTest.set_index('timestamp')\n",
    "# Calculating original consumption value from per meter consumption and number of meters\n",
    "preTest.Per_meter_comsumption_with_inter  = preTest.Per_meter_comsumption_with_inter * test_data_scaling_info.meters\n",
    "# Storing results in CSV\n",
    "preTest[['Per_meter_comsumption_with_inter']].to_csv('WaterDMA_1.csv',index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
