{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HeatDMA1 import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "sns.set()\n",
    "\n",
    "\n",
    "# Heat data set paths\n",
    "HeatDMA_Number_of_Meters_path = '/Users/jagrutigodambe/Desktop/Data/heat/HeatDMA_Number_of_Meters.csv'\n",
    "Training_HeatDMA_path = '/Users/jagrutigodambe/Desktop/Data/heat/Training_HeatDMA.csv'\n",
    "Testing_HeatDMA_path = '/Users/jagrutigodambe/Desktop/Data/heat/Testing_HeatDMA.csv'\n",
    "Weather_data_path = '/Users/jagrutigodambe/Desktop/Data/weather/Weather_Bronderslev_20152022.csv'\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LX2XQ827nat_"
   },
   "outputs": [],
   "source": [
    "# Initialize the data preprocessing with file paths\n",
    "prepro = Heat_data_preprocessing(Training_HeatDMA_path, \n",
    "                        HeatDMA_Number_of_Meters_path,\n",
    "                        Weather_data_path)\n",
    "\n",
    "# Fit the preprocessing on training data and transform it, then apply the same transformation to testing data\n",
    "train_data = prepro.fit()\n",
    "test_data = prepro.transform(Testing_HeatDMA_path)\n",
    "\n",
    "\n",
    "train_data_scaling_info = train_data[['comsumption','meters']]\n",
    "test_data_scaling_info = test_data[['comsumption','meters']]\n",
    "\n",
    "train_data = train_data.drop(['comsumption','meters'], axis=1)\n",
    "test_data = test_data.drop(['comsumption','meters'], axis=1)\n",
    "\n",
    "\n",
    "with open(\"scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(prepro.get_scaler(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hRnzqHunnl23",
    "outputId": "3a0b790c-10b6-470c-df48-6989d03a4e89"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26088/26088 [00:02<00:00, 11517.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create sequences of 192 entries to predict the next 24 entries\n",
    "\n",
    "sequence_length_x = 192\n",
    "sequence_length_y = 24 # y is label\n",
    "sequences, labels , train_scaling_info  = create_sequences(train_data, sequence_length_x, sequence_length_y, train_data_scaling_info)\n",
    "train_X, train_y, train_scale_info, test_X, test_y, test_scale_info = train_test_split(sequences, labels , train_scaling_info, train_size = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model ,data):#, scaling_info):\n",
    "    with torch.no_grad():\n",
    "       data = torch.Tensor(data)\n",
    "       prediction = model(data)\n",
    "    return pd.DataFrame(prediction.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_hqkywlrn0mj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters, create data loaders, and initialize the model and optimizer\n",
    "input_size = 23 # Feature dimension\n",
    "sequence_length = 192\n",
    "\n",
    "hidden_size = 128\n",
    "num_layers = 5\n",
    "\n",
    "output_size = 24\n",
    "\n",
    "\n",
    "num_epochs = 500\n",
    "batch_size = 32\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "device = get_device()\n",
    "print(device)\n",
    "\n",
    "\n",
    "class HeatData(Dataset):\n",
    "\n",
    "    def __init__(self, sequences, labels):\n",
    "      self.sequences = sequences\n",
    "      self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "      return self.sequences.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      return torch.Tensor(self.sequences[idx]), torch.Tensor(self.labels[idx])\n",
    "\n",
    "\n",
    "heatData_train = HeatData(train_X, train_y)\n",
    "heatData_val = HeatData(test_X, test_y)\n",
    "\n",
    "train_dataloader = DataLoader(heatData_train, batch_size=batch_size, shuffle = True)\n",
    "test_dataloader = DataLoader(heatData_val, batch_size=batch_size, shuffle = False)\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, output_size, num_layers)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model \n",
    "\n",
    "list_train_loss = []\n",
    "list_val_loss = []\n",
    "\n",
    "prev_val_loss = 1000 \n",
    "epochs = 50\n",
    "for e in tqdm(range(epochs)):#\n",
    "\n",
    "    model.train()\n",
    "    for data, labels in train_dataloader:\n",
    "        if torch.cuda.is_available():\n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        target = model(data)\n",
    "        loss = criterion(target, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    list_train_loss.append(loss.item())\n",
    "\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad(): \n",
    "        for data, labels in test_dataloader:\n",
    "            if torch.cuda.is_available():\n",
    "                data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "            target = model(data)\n",
    "            loss = criterion(target, labels)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    valid_loss = valid_loss / len(test_dataloader)\n",
    "    list_val_loss.append(valid_loss)\n",
    "    \n",
    "\n",
    "    if prev_val_loss > valid_loss:\n",
    "        prev_val_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model_daylight{}.pth'.format(e))\n",
    "\n",
    "    print(f'Epoch {e+1} \\t\\t Training Loss: {loss.item()} \\t\\t Validation Loss: {valid_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained LSTM model weights \n",
    "model = LSTMModel(input_size, hidden_size, output_size, num_layers\n",
    "model.load_state_dict(torch.load('model_daylight{}.pth'.format(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions and calculate MAE and MAPE.\n",
    "predictions = predict(model, test_X)\n",
    "prediction_for_all_meter = pd.DataFrame(test_scale_info[:,:,1]) * predictions\n",
    "\n",
    "mean_absolute_error(test_scale_info[:,:,0],prediction_for_all_meter.to_numpy())\n",
    "\n",
    "mean_absolute_percentage_error(test_scale_info[:,:,0],prediction_for_all_meter.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and fill missing values in the test data using the model.\n",
    "def test_pred(model, test_data):\n",
    "    torch.no_grad()\n",
    "    test_data = test_data.reset_index()\n",
    "    \n",
    "    idx = test_data[test_data.Per_meter_comsumption_with_inter.isna()].timestamp.dt.date.drop_duplicates(keep='first').index.tolist()\n",
    "    print(idx)\n",
    "    for i in idx:\n",
    "        pre = model(torch.Tensor((test_data.loc[i-192\n",
    "              :i-1]).drop('timestamp', axis=1).values).reshape(1,192,23))\n",
    "       #print((pre.detach().numpy().reshape(-1,1)))\n",
    "        #print(len(test_data.loc[i :i+23,'Per_meter_comsumption_with_inter']))\n",
    "        test_data.loc[i :i+23,'Per_meter_comsumption_with_inter'] = pre.detach().numpy().reshape(-1,1)\n",
    "    \n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preTest = test_pred(model, test_data)\n",
    "preTest= preTest.set_index('timestamp')\n",
    "# Calculating original consumption value from per meter consumption and number of meters\n",
    "preTest.Per_meter_comsumption_with_inter  = preTest.Per_meter_comsumption_with_inter * test_data_scaling_info.meters\n",
    "# Storing results in CSV\n",
    "preTest[['Per_meter_comsumption_with_inter']].to_csv('HeatDMA1_Predictions.csv',index=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
