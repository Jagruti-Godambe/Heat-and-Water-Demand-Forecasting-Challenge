# Time Series Forecasting Project
# This project involves time series forecasting for heat and water demand in different regions. The notebook consists of steps for data preprocessing, sequence generation, model training, and evaluation using LSTM-based models.

# Importing Necessary Libraries
# We begin by importing necessary libraries for data handling, visualization, and model building.

from HeatDMA1 import *
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from torch.utils.data import Dataset, DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
from sklearn.preprocessing import MinMaxScaler
import pickle
sns.set()

# Defining File Paths and Setting Random Seeds
# We define the file paths for the dataset and set random seeds for reproducibility.

# Heat data set paths
HeatDMA_Number_of_Meters_path = '/path/to/HeatDMA_Number_of_Meters.csv'
Training_HeatDMA_path = '/path/to/Training_HeatDMA.csv'
Testing_HeatDMA_path = '/path/to/Testing_HeatDMA.csv'
Weather_data_path = '/path/to/Weather_Bronderslev_20152022.csv'

# Setting seeds for reproducibility
torch.manual_seed(0)
np.random.seed(0)

# Data Preprocessing
# We initialize and run the data preprocessing on the training and testing datasets. This step also includes saving the scaling information for future use.

# Initialize the data preprocessing with file paths
prepro = Heat_data_preprocessing(Training_HeatDMA_path, HeatDMA_Number_of_Meters_path, Weather_data_path)

# Preprocessing training and testing data
train_data = prepro.fit()
test_data = prepro.transform(Testing_HeatDMA_path)

# Extracting and removing scaling information from data
train_data_scaling_info = train_data[['comsumption','meters']]
test_data_scaling_info = test_data[['comsumption','meters']]
train_data = train_data.drop(['comsumption','meters'], axis=1)
test_data = test_data.drop(['comsumption','meters'], axis=1)

# Saving the scaler for later use
with open("scaler.pkl", "wb") as f:
    pickle.dump(prepro.get_scaler(), f)

# Sequence Generation
# We create sequences of data to be used as input (X) and labels (y) for the model. The sequence length for input is 192, and for labels, it's 24.

# Sequence length configuration
sequence_length_x = 192
sequence_length_y = 24  # Number of future points to predict

# Creating sequences and splitting the data into training and testing sets
sequences, labels, train_scaling_info = create_sequences(train_data, sequence_length_x, sequence_length_y, train_data_scaling_info)
train_X, train_y, train_scale_info, test_X, test_y, test_scale_info = train_test_split(sequences, labels, train_scaling_info, train_size=0.8)

# Model Setup
# Here we define the model, set hyperparameters, and initialize the dataset and data loaders for training.

# Hyperparameter definition
input_size = 23  # Number of features
hidden_size = 128
num_layers = 5
output_size = 24  # Number of future steps to predict
batch_size = 32
learning_rate = 0.001
num_epochs = 500

# Get the device (CPU or GPU)
device = get_device()
print(device)

# Define the custom Dataset class for handling sequences and labels
class HeatData(Dataset):
    def __init__(self, sequences, labels):
        self.sequences = sequences
        self.labels = labels

    def __len__(self):
        return self.sequences.shape[0]

    def __getitem__(self, idx):
        return torch.Tensor(self.sequences[idx]), torch.Tensor(self.labels[idx])

# Creating data loaders
heatData_train = HeatData(train_X, train_y)
heatData_val = HeatData(test_X, test_y)
train_dataloader = DataLoader(heatData_train, batch_size=batch_size, shuffle=True)
test_dataloader = DataLoader(heatData_val, batch_size=batch_size, shuffle=False)

# Initializing the LSTM model, loss function, and optimizer
model = LSTMModel(input_size, hidden_size, output_size, num_layers)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Model Training
# This section trains the LSTM model, evaluates it on the validation set, and saves the best-performing model.

# Training the model
list_train_loss = []
list_val_loss = []

prev_val_loss = 1000  # To track the best validation loss

for e in tqdm(range(num_epochs)):
    model.train()
    for data, labels in train_dataloader:
        if torch.cuda.is_available():
            data, labels = data.cuda(), labels.cuda()
        
        optimizer.zero_grad()
        target = model(data)
        loss = criterion(target, labels)
        loss.backward()
        optimizer.step()

    list_train_loss.append(loss.item())

    # Validation step
    valid_loss = 0.0
    model.eval()
    with torch.no_grad():
        for data, labels in test_dataloader:
            if torch.cuda.is_available():
                data, labels = data.cuda(), labels.cuda()
            target = model(data)
            loss = criterion(target, labels)
            valid_loss += loss.item()

    valid_loss /= len(test_dataloader)
    list_val_loss.append(valid_loss)

    # Save the best model
    if prev_val_loss > valid_loss:
        prev_val_loss = valid_loss
        torch.save(model.state_dict(), f'model_daylight{e}.pth')

    print(f'Epoch {e+1} \t Training Loss: {loss.item()} \t Validation Loss: {valid_loss}')

# Model Evaluation and Prediction
# We load the trained model and use it to generate predictions on the test set, then calculate MAE and MAPE.

# Loading the trained model
model.load_state_dict(torch.load('model_daylight3.pth'))

# Generating predictions
predictions = predict(model, test_X)
prediction_for_all_meter = pd.DataFrame(test_scale_info[:,:,1]) * predictions

# Evaluating the performance
mae = mean_absolute_error(test_scale_info[:,:,0], prediction_for_all_meter.to_numpy())
mape = mean_absolute_percentage_error(test_scale_info[:,:,0], prediction_for_all_meter.to_numpy())

print(f"Mean Absolute Error: {mae}")
print(f"Mean Absolute Percentage Error: {mape}")

# Predicting Missing Values
# Using the trained model, we fill in the missing values in the test data.

def test_pred(model, test_data):
    torch.no_grad()
    test_data = test_data.reset_index()
    
    idx = test_data[test_data.Per_meter_comsumption_with_inter.isna()].timestamp.dt.date.drop_duplicates(keep='first').index.tolist()
    print(idx)
    for i in idx:
        pre = model(torch.Tensor((test_data.loc[i-192 :i-1]).drop('timestamp', axis=1).values).reshape(1,192,23))
        test_data.loc[i :i+23,'Per_meter_comsumption_with_inter'] = pre.detach().numpy().reshape(-1,1)
    
    return test_data

# Fill missing values
preTest = test_pred(model, test_data)
preTest = preTest.set_index('timestamp')

# Calculating original consumption value from per meter consumption and number of meters
preTest['Per_meter_comsumption_with_inter'] = preTest['Per_meter_comsumption_with_inter'] * test_data_scaling_info.meters

# Storing results in CSV
preTest[['Per_meter_comsumption_with_inter']].to_csv('HeatDMA1_Predictions.csv', index=True)
