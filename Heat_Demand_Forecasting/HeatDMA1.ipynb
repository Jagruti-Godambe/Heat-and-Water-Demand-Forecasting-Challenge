{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecasting Project\n",
    "This project involves time series forecasting for heat and water demand in different regions. The notebook consists of steps for data preprocessing, sequence generation, model training, and evaluation using LSTM-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HeatDMA1 import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining File Paths and Setting Random Seeds\n",
    "We define the file paths for the dataset and set random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heat data set paths\n",
    "HeatDMA_Number_of_Meters_path = '/path/to/HeatDMA_Number_of_Meters.csv'\n",
    "Training_HeatDMA_path = '/path/to/Training_HeatDMA.csv'\n",
    "Testing_HeatDMA_path = '/path/to/Testing_HeatDMA.csv'\n",
    "Weather_data_path = '/path/to/Weather_Bronderslev_20152022.csv'\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "We initialize and run the data preprocessing on the training and testing datasets. This step also includes saving the scaling information for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data preprocessing with file paths\n",
    "prepro = Heat_data_preprocessing(Training_HeatDMA_path, HeatDMA_Number_of_Meters_path, Weather_data_path)\n",
    "\n",
    "# Preprocessing training and testing data\n",
    "train_data = prepro.fit()\n",
    "test_data = prepro.transform(Testing_HeatDMA_path)\n",
    "\n",
    "# Extracting and removing scaling information from data\n",
    "train_data_scaling_info = train_data[['comsumption','meters']]\n",
    "test_data_scaling_info = test_data[['comsumption','meters']]\n",
    "train_data = train_data.drop(['comsumption','meters'], axis=1)\n",
    "test_data = test_data.drop(['comsumption','meters'], axis=1)\n",
    "\n",
    "# Saving the scaler for later use\n",
    "with open(\"scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(prepro.get_scaler(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Generation\n",
    "We create sequences of data to be used as input (X) and labels (y) for the model. The sequence length for input is 192, and for labels, it's 24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence length configuration\n",
    "sequence_length_x = 192\n",
    "sequence_length_y = 24  # Number of future points to predict\n",
    "\n",
    "# Creating sequences and splitting the data into training and testing sets\n",
    "sequences, labels, train_scaling_info = create_sequences(train_data, sequence_length_x, sequence_length_y, train_data_scaling_info)\n",
    "train_X, train_y, train_scale_info, test_X, test_y, test_scale_info = train_test_split(sequences, labels, train_scaling_info, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "Here we define the model, set hyperparameters, and initialize the dataset and data loaders for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter definition\n",
    "input_size = 23  # Number of features\n",
    "hidden_size = 128\n",
    "num_layers = 5\n",
    "output_size = 24  # Number of future steps to predict\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 500\n",
    "\n",
    "# Get the device (CPU or GPU)\n",
    "device = get_device()\n",
    "print(device)\n",
    "\n",
    "# Define the custom Dataset class for handling sequences and labels\n",
    "class HeatData(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sequences.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.Tensor(self.sequences[idx]), torch.Tensor(self.labels[idx])\n",
    "\n",
    "# Creating data loaders\n",
    "heatData_train = HeatData(train_X, train_y)\n",
    "heatData_val = HeatData(test_X, test_y)\n",
    "train_dataloader = DataLoader(heatData_train, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(heatData_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initializing the LSTM model, loss function, and optimizer\n",
    "model = LSTMModel(input_size, hidden_size, output_size, num_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "This section trains the LSTM model, evaluates it on the validation set, and saves the best-performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "list_train_loss = []\n",
    "list_val_loss = []\n",
    "\n",
    "prev_val_loss = 1000  # To track the best validation loss\n",
    "\n",
    "for e in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    for data, labels in train_dataloader:\n",
    "        if torch.cuda.is_available():\n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        target = model(data)\n",
    "        loss = criterion(target, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    list_train_loss.append(loss.item())\n",
    "\n",
    "    # Validation step\n",
    "    valid_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_dataloader:\n",
    "            if torch.cuda.is_available():\n",
    "                data, labels = data.cuda(), labels.cuda()\n",
    "            target = model(data)\n",
    "            loss = criterion(target, labels)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    valid_loss /= len(test_dataloader)\n",
    "    list_val_loss.append(valid_loss)\n",
    "\n",
    "    # Save the best model\n",
    "    if prev_val_loss > valid_loss:\n",
    "        prev_val_loss = valid_loss\n",
    "        torch.save(model.state_dict(), f'model_daylight{e}.pth')\n",
    "\n",
    "    print(f'Epoch {e+1} \\
